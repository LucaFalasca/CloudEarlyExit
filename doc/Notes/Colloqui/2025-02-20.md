
> [!TODO] Scaling min-max della funzione obiettivo
> - Fatto per la latenza
> - da controllare per l'energia

> [!Done] Test ottimizzatore
> Sembra funzionare.
> Imponendo che l'output deve essere rimandato al server che fa partire l'inferenza, la divisione è fatta in automatico; altrimenti controllare su [[Ottimizzazione#^91b978]] i parametri di test.
> Il test è sempre fatto sul modellino piccolo estratto da ResNet50

> [!Todo] Ragionare su un'ottimizzazione a due livelli
> Si fa prima un'ottimizzazione in funzione dell'energia e della latenza e poi si fa una seconda fase di ottimizzazione per la quantizzazione del modello

> [!Todo] Valutare Effetto della Quantizzazione sulla prima parte del modello
> La prima parte del modello è quella che viene assegnata al server di inferenza (nodo edge con requisito energetico più importante)

> [!Todo] Uso QuantizeLinear e Dequantize Linear
> Si potrebbe pensare di usarli come dei livelli di compressione dei dati per inviare un numero inferiori di dati
> Attenzione. Come si può calibrare in questo caso??

> [!Todo] Quantizzazione TFLite
> Verificare che la quantizzazione sia più veloce con dei batch piuttosto che con singoli elementi





> [!NOTE] Title
> Contents

